# Review Delight Point Extractor

## Table of Contents

- [Introduction](#introduction)
- [Features](#features)
- [Setup Instructions](#setup-instructions)
- [Usage Examples](#usage-examples)
- [Attribute Extraction Approach](#attribute-extraction-approach)
- [Clustering and Deduplication Methodology](#clustering-and-deduplication-methodology)
- [Evaluation Component](#evaluation-component)
- [Limitations and Potential Improvements](#limitations-and-potential-improvements)
- [Assumptions](#assumptions)
- [Known Issues](#known-issues)

## Introduction

This CLI tool analyzes customer reviews from a JSON file to extract key positive product attributes. It then groups semantically similar attributes and ranks them by frequency, providing valuable insights for marketing and product development. The tool also includes an evaluation mechanism to assess its accuracy against a ground truth dataset.

## Features

- **Attribute Extraction**: Identifies positive product attributes from review text using OpenAI's GPT-3.5-turbo model.
- **Attribute Clustering**: Groups semantically similar attributes using TF-IDF vectorization and K-Means clustering.
- **Frequency Ranking**: Ranks extracted attributes based on their frequency of mention.
- **Input/Output Handling**: Processes JSON input files and generates JSON and CSV output files.
- **Evaluation**: Assesses the tool's performance against a provided evaluation dataset, reporting accuracy metrics.



## Setup Instructions

To set up and run the Review Delight Point Extractor, follow these steps:

1.  **Clone the repository (if applicable):**

    ```bash
    git clone <repository_url>
    cd Review-Attribute-Analyzer
    ```

    *(Note: If you received the project as a direct download, you can skip this step and navigate to the project directory.)*

2.  **Install Dependencies:**

    Ensure you have Python 3.13 or higher installed. Then, install the required Python packages using pip:

    ```bash
    pip install -r requirements.txt
    ```

    The `requirements.txt` file contains the following dependencies:
    - `pandas`: For data manipulation and CSV output.
    - `scikit-learn`: For TF-IDF vectorization and K-Means clustering.
    - `numpy`: A fundamental package for numerical computation.
    - `openai`: The official OpenAI Python client library for interacting with the GPT-3.5-turbo model.

3.  **Set up OpenAI API Key:**

    The tool requires an OpenAI API key to extract attributes from reviews. You can obtain an API key from the [OpenAI platform](https://platform.openai.com/account/api-keys). Once you have your key, you can provide it to the tool in one of two ways:

    -   **As a command-line argument:** Use the `--openai_api_key` argument when running the script.
        Example: `python main.py --reviews_file reviews.json --openai_api_key YOUR_API_KEY`

    -   **As an environment variable:** Set the `OPENAI_API_KEY` environment variable before running the script.
        Example:
        ```bash
        setx OPENAI_API_KEY "YOUR_API_KEY"
        python main.py --reviews_file reviews.json
        ```

    It is recommended to use environment variables for security reasons, especially in production environments.
    



## Usage Examples

### 1. Process Reviews and Generate Outputs

To process a JSON file containing customer reviews and generate the output JSON and CSV files, use the following command:
```bash
python main.py \
    --reviews_file reviews.json \
    --output_json_file output_reviews.json \
    --output_csv_file ranked_attributes.csv \
    --openai_api_key YOUR_OPENAI_API_KEY
```

-   `--reviews_file`: (Required) Path to your input JSON file containing customer reviews. (e.g., `reviews.json`)
-   `--output_json_file`: (Optional) Path where the processed reviews (with extracted attributes) will be saved as a JSON file. Defaults to `output_reviews.json`.
-   `--output_csv_file`: (Optional) Path where the ranked attributes and their frequencies will be saved as a CSV file. Defaults to `ranked_attributes.csv`.
-   `--openai_api_key`: (Optional if already in environment variables) Your OpenAI API key. Alternatively, you can set the `OPENAI_API_KEY` environment variable.

NOTE: Any input files given by user should be located in project `data/` directory and files generated by algorithm will be generated at `output/` directory.

**Example Input (`data/reviews.json`):**

```json
[
    {
        "review_id": "a2b077c7-4366-4c61-a1fe-cecf0e63a309",
        "author": "S Mal",
        "body": "I have ordered it 4 times & will continue to till 400 coz it\'s so good. The smell is to die for. It\'s so fresh ❤️"
    },
    {
        "review_id": "b3c188d8-5477-5d72-b2ff-dfd11f74b410",
        "author": "J Doe",
        "body": "Great product, very effective and gentle on skin."
    }
]
```

**Example Output 1 (`output/output_reviews.json`):**

```json
{
    "reviews": [
        {
            "review_id": "a2b077c7-4366-4c61-a1fe-cecf0e63a309",
            "author": "S Mal",
            "body": "I have ordered it 4 times & will continue to till 400 coz it\'s so good. The smell is to die for. It\'s so fresh ❤️",
            "delight_attributes": ["Fragrance", "Freshness"]
        },
        {
            "review_id": "b3c188d8-5477-5d72-b2ff-dfd11f74b410",
            "author": "J Doe",
            "body": "Great product, very effective and gentle on skin.",
            "delight_attributes": ["Effectiveness", "Gentleness"]
        }
    ]
}
```

**Example Output 2 (`output/ranked_attributes.csv`):**

```csv
Delight Attribute,Frequency
Fragrance,1
Freshness,1
Effectiveness,1
Gentleness,1
```

### 2. Evaluate the Tool

To evaluate the performance of the attribute extraction, you can provide an evaluation CSV file. This will print performance metrics to the console.

```bash
python review_delight_extractor.py \
    --reviews_file reviews.json \
    --output_json_file output_reviews.json \
    --output_csv_file ranked_attributes.csv \
    --evaluate_file delight-evaluation.csv \
    --openai_api_key YOUR_OPENAI_API_KEY
```

-   `--evaluate_file`: (Optional) Path to your evaluation CSV file. (e.g., `delight-evaluation.csv`)

**Example Input (`data/delight-evaluation.csv`):**

```csv
review_id,expected_attributes
a2b077c7-4366-4c61-a1fe-cecf0e63a309,"Fragrance, Freshness"
b3c188d8-5477-5d72-b2ff-dfd11f74b410,"Effectiveness, Skin-friendly"
```

**Example Console Output (Evaluation Results):**

```
--- Evaluation Results ---
Number of correct attribute extractions: X
Number of incorrect attribute extractions: Y
Overall accuracy percentage: Z.YY%
```




## Attribute Extraction Approach

The core of this tool's attribute extraction capability relies on the advanced natural language processing (NLP) power of OpenAI's GPT-3.5-turbo model. Instead of using rule-based systems or traditional machine learning models that require extensive training data and feature engineering, we leverage a pre-trained large language model (LLM) to identify positive product attributes directly from customer review text.

### Why GPT-3.5-turbo?

GPT-3.5-turbo is chosen for its ability to understand context, nuances, and sentiment in human language. This allows for a more flexible and robust extraction process compared to methods that might struggle with variations in phrasing or informal language commonly found in reviews. The model can infer the 'delight points' even if they are not explicitly stated as single keywords, but rather implied through descriptive sentences.

### How it Works

1.  **Prompt Engineering**: For each review, a specific prompt is crafted and sent to the OpenAI API. This prompt instructs the model to act as a helpful assistant whose sole purpose is to extract positive product attributes from the given review. It also specifies the desired output format: a comma-separated list of attributes, or 'None' if no positive attributes are found.

    The system message provided to the model is:
    ```
    You are a helpful assistant that extracts positive product attributes from customer reviews. 
    Extract positive product attributes from reviews. Use specific standardized terms:
    - Fragrance (smell/scent), Quality (build/material), Longevity (long-lasting), Effectiveness (works well)
    - Packaging, Moisturizing, Non-allergenic, Climate Suitability, Compatibility, Texture
    - Overall Satisfaction, Customer Service, Overall Quality

    Return comma-separated list. Examples:
    "smell is amazing" → Fragrance
    "good quality, nice packaging, lasts all day" → Quality, Packaging, Longevity
    "satisfied with product" → Overall Satisfaction
    If no positive attributes are found, respond with 'None'.
    ```

    The user message then contains the review body:
    ```
    Extract positive product attributes from this review: '{review_body}'
    ```

2.  **API Interaction**: The `review_delight_extractor.py` script makes an API call to OpenAI's `chat.completions.create` endpoint. The `review_body` from each customer review is passed as part of the user message.

3.  **Attribute Parsing**: The response from the OpenAI API is a string containing the extracted attributes. This string is then parsed by splitting it by commas and stripping any leading or trailing whitespace from each attribute. If the model responds with 'None' (case-insensitive), an empty list is returned, indicating no positive attributes were found.

### Advantages of this Approach

-   **Semantic Understanding**: The LLM can understand the meaning and context of words, allowing it to identify attributes even when expressed indirectly or metaphorically.
-   **Reduced Development Time**: Leverages a powerful pre-trained model, significantly reducing the need for manual data labeling and model training.
-   **Flexibility**: Easily adaptable to different product types and review styles without major code changes, primarily by refining the prompt.
-   **Scalability**: OpenAI's API is designed for high-volume requests, making it suitable for processing large datasets of reviews.

### Considerations

-   **API Cost**: Each API call incurs a cost, which can accumulate when processing a very large number of reviews. Strategies like batching requests or filtering reviews before sending them to the API can help mitigate this.
-   **Rate Limits**: OpenAI APIs have rate limits, which need to be managed to prevent errors during high-volume processing. The current implementation does not include explicit rate limiting, which could be a future improvement.
-   **Dependence on External Service**: The tool's core functionality relies on the availability and performance of the OpenAI API.
-   **Consistency**: While powerful, LLMs can sometimes produce inconsistent results. The prompt engineering aims to guide the model towards consistent output, but occasional variations are possible.

This approach prioritizes leveraging state-of-the-art NLP capabilities for accurate and efficient attribute extraction, minimizing the complexity of building and maintaining a custom NLP model.




## Clustering and Deduplication Methodology

The process of clustering and deduplicating extracted attributes is crucial for transforming raw, potentially redundant, attributes into meaningful and actionable insights. Customer reviews often use varied phrasing to describe the same positive aspect of a product (e.g., "great smell," "lovely scent," "amazing fragrance"). Without clustering, these semantically similar attributes would be treated as distinct, leading to fragmented insights and inaccurate frequency counts.

### Objectives of Clustering

1.  **Consolidation**: Grouping together attributes that convey the same underlying sentiment or feature.
2.  **Deduplication**: Reducing redundancy by identifying and merging equivalent attributes.
3.  **Insight Generation**: Providing a clearer, more concise summary of what customers love, making it easier to identify top delight points.

### Methodology: TF-IDF and K-Means Clustering
This tool employs a combination of TF-IDF (Term Frequency-Inverse Document Frequency) for text vectorization and K-Means for clustering. This is a common and effective approach for grouping text data based on semantic similarity.

1.  **TF-IDF Vectorization**: 
    -   **Purpose**: To convert the extracted text attributes into numerical vectors that can be processed by machine learning algorithms. TF-IDF assigns a weight to each word that reflects its importance in a document (in this case, an attribute) relative to a collection of documents (all extracted attributes).
    -   **Process**: Each unique attribute extracted from the reviews is treated as a 'document'. The `TfidfVectorizer` from `scikit-learn` is used to transform these attributes into a matrix of TF-IDF features. Attributes that are semantically similar will have similar TF-IDF vectors.

2.  **K-Means Clustering**: 
    -   **Purpose**: To partition the TF-IDF vectors into a predefined number of clusters, where each cluster represents a group of semantically similar attributes.
    -   **Process**: The `KMeans` algorithm from `scikit-learn` is applied to the TF-IDF matrix. K-Means aims to partition `n` observations into `k` clusters in which each observation belongs to the cluster with the nearest mean (centroid), serving as a prototype of the cluster.
    -   **Determining `num_clusters`**: In the current implementation, the number of clusters (`num_clusters`) is determine by the Elbow Method. This is a heuristic method to find a suitable number of clusters for K-Means clustering.

3.  **Representative Attribute Selection**: 
    -   **Purpose**: After clustering, each cluster contains several attributes that are considered semantically similar. To deduplicate and provide a concise representation for each cluster, a single representative attribute is chosen.
    -   **Process**: For each cluster, the most frequently occurring attribute within that cluster is selected as its representative/label. This approach assumes that the most frequent attribute is the most common or accurate representation of the sentiment expressed by that cluster. While simple, it effectively consolidates similar terms into a single, dominant attribute for reporting.


### Limitations and Future Improvements

-   **Semantic Equivalence**: The evaluation criteria mention that "semantically equivalent attributes should be considered correct matches." While TF-IDF and K-Means help group similar terms, true semantic equivalence can be challenging. Future improvements could involve using more advanced NLP techniques for semantic similarity, such as word embeddings (e.g., Word2Vec, FastText) or contextual embeddings (e.g., BERT, Sentence-BERT), to achieve more precise clustering and evaluation.
-   **Cluster Naming**: The current implementation names clusters generically (e.g., "Cluster 0", "Cluster 1"). A more user-friendly approach would be to automatically generate descriptive names for clusters based on their content, perhaps by identifying common keywords or phrases within each cluster.
-   **Handling Outliers**: K-Means is sensitive to outliers. Reviews that contain very unique or rare attributes might form small, isolated clusters or be incorrectly grouped. Techniques like DBSCAN or hierarchical clustering could be explored for better handling of such cases.

By employing TF-IDF and K-Means, the tool effectively transforms disparate attribute mentions into consolidated, actionable insights, providing a clearer picture of customer delight points.




## Evaluation Component

The evaluation component of the Review Delight Point Extractor is designed to provide a quantitative measure of the tool's accuracy in extracting relevant attributes from customer reviews. This is crucial for understanding the reliability of the extracted insights and for identifying areas for improvement.

### Input: Evaluation Dataset (`data/delight-evaluation.csv`)

The evaluation process relies on a ground truth dataset provided in a CSV file (e.g., `delight-evaluation.csv`). This file contains a list of `review_id`s and their corresponding `delight_attributes`. The `delight_attributes` are a comma-separated list of attributes that are considered correct for that particular review, based on human annotation or a predefined standard.

**Example `delight-evaluation.csv`:**

```csv
review_id,author,body,delight_attribute
a2b077c7-4366-4c61-a1fe-cecf0e63a309,S Mal,"I have ordered it 4 times & will continue to till 400 coz it's so good. The smell is to die for. It's so fresh ❤️",Fragrance
8cd3005d-f39c-4a28-88f2-4f0effdd01e5,Ruqayyah Asiri,"I purchased the small size just for trial and I'm very happy with quality of the product. I again got the bigger size i loved the packaging. The deodorant smells very nice and its effect last for whole day and it also moisturized the skin and give very smooth feeling of that area. I highly recommend it","Quality, Packaging, Fragrance, Longevity, Moisturizing"
```

### Process

1.  **Load Data**: The `evaluate` method in `ReviewDelightExtractor` loads the `delight-evaluation.csv` file and the `output_reviews.json` file (which contains the attributes extracted by the tool).

2.  **Attribute Comparison**: For each review in the evaluation dataset, the tool compares the `expected_attributes` with the `delight_attributes` that were extracted by the tool for that same review.

3.  **Matching Logic (Simple Semantic Equivalence)**:
    The current implementation uses a simplified approach to determine if an extracted attribute is a correct match. It checks for semantic equivalence by performing a case-insensitive substring check. If an `expected` attribute is found within an `extracted` attribute, or vice-versa, it is considered a match.

    **Example**: If the `expected_attributes` include "Fragrance" and the tool extracts "Good Smell", this would currently **not** be counted as a correct match unless one string is a substring of the other. The problem statement explicitly mentions: "If expected is 'Fragrance' and extracted is 'Good Smell', this should count as correct." This highlights a limitation of the current simple substring matching, which is addressed in the "Limitations and Potential Improvements" section.

4.  **Metric Calculation**: Based on the comparisons, the tool calculates:
    -   **Number of correct attribute extractions**: The count of instances where at least one extracted attribute for a review semantically matched an expected attribute.
    -   **Number of incorrect attribute extractions**: The count of instances where no extracted attribute for a review semantically matched any expected attribute.
    -   **Overall accuracy percentage**: Calculated as `(correct_extractions / total_extractions) * 100`.


### Output: Performance Metrics

The evaluation results are printed to the console, providing a quick overview of the tool's performance:

```
--- Evaluation Results ---
Number of correct attribute extractions: X
Number of incorrect attribute extractions: Y
Overall accuracy percentage: Z.YY%
```

## Limitations and Potential Improvements

While the Review Delight Point Extractor provides a solid foundation for analyzing customer reviews, there are several areas where it can be improved to enhance its accuracy, robustness, and user-friendliness. These improvements address both the underlying methodologies and practical considerations for deployment.


### 1. Enhanced Attribute Extraction

-   **Fine-tuned LLM or Domain-Specific Models**: Currently, the tool relies on a general-purpose LLM (GPT-3.5-turbo). For higher accuracy and more nuanced attribute extraction in specific product domains, fine-tuning a smaller LLM on a dataset of product reviews or using domain-specific NLP models could yield better results. This would reduce reliance on prompt engineering and potentially lower API costs.
-   **Contextual Extraction**: The current extraction is largely based on individual review bodies. Incorporating metadata like product categories, user demographics, or review ratings could provide additional context for more precise attribute identification.
-   **Multi-attribute Extraction Refinement**: While the prompt asks for a comma-separated list, some reviews might contain multiple positive attributes that are difficult for the LLM to separate distinctly. Implementing post-processing steps or refining the prompt to encourage more granular attribute separation could be beneficial.


### 2. Advanced Clustering and Deduplication

-   **Semantic Similarity with Embeddings**: The current clustering uses TF-IDF, which captures word importance but not deep semantic meaning. Utilizing word embeddings (e.g., Word2Vec, GloVe) or contextual embeddings (e.g., Sentence-BERT) combined with cosine similarity would allow for more accurate grouping of semantically equivalent attributes, even if they use entirely different vocabulary (e.g., "good smell" and "pleasant aroma").
-   **Hierarchical Clustering or DBSCAN**: For datasets with varying densities of attributes or to identify natural groupings without pre-specifying the number of clusters, exploring hierarchical clustering or DBSCAN could be valuable alternatives to K-Means.

### 3. Robust Evaluation Metrics

-   **Precision, Recall, and F1-Score**: Moving beyond simple correct/incorrect counts, implementing standard NLP evaluation metrics like precision, recall, and F1-score would provide a more comprehensive assessment of the extraction and clustering performance. This would involve comparing the extracted attributes against the ground truth at a more granular level.
-   **Human-in-the-Loop Evaluation**: For continuous improvement, a system that allows human annotators to review and correct extracted attributes and cluster groupings would be invaluable. This feedback loop could then be used to fine-tune the LLM or refine clustering algorithms.

### 4. Performance and Scalability

-   **Batch Processing**: Optimizing the API calls to send reviews in batches could reduce overhead and improve efficiency.
-   **Caching**: Caching previously extracted attributes for identical review bodies would prevent redundant API calls and save costs.
-   **Error Handling and Retry Mechanisms**: More sophisticated error handling, including exponential backoff and retry mechanisms for API calls, would make the tool more robust to transient network issues or API rate limits.

### 5. User Experience and Deployment

-   **Web Interface**: Developing a simple web interface would make the tool accessible to non-technical users, allowing them to upload review files and view results in a more visual format.
-   **Database Integration**: For persistent storage of reviews and extracted attributes, integrating with a database (e.g., SQLite for local use) would be beneficial.

By addressing these areas, the Review Delight Point Extractor can evolve into a more powerful, accurate, and user-friendly tool for extracting valuable insights from customer feedback.



## Assumptions

During the development of the Review Delight Point Extractor, the following assumptions were made:

1.  **Input JSON Format**: The tool assumes that the input review data is provided in a JSON file with the structure `[...]` where each item is an object containing at least a `body` field with the review text and a `review_id` field for unique identification. The presence of `author` and other fields is expected but not strictly required for the core processing logic.

2.  **Evaluation CSV Format**: The evaluation dataset is expected to be in a CSV file with columns `review_id` and `delight_attribute`. The `delight_attribute` column is assumed to contain a comma-separated string of ground truth attributes for each review.

3.  **OpenAI API Availability and Functionality**: The tool assumes that the OpenAI API is accessible and that the `gpt-3.5-turbo` model is available and functions as expected for extracting attributes based on the provided prompt.

4.  **Positive Attribute Focus**: The tool is specifically designed to extract *positive* product attributes. It does not attempt to identify negative feedback or neutral comments.

5.  **Evaluation Matching Simplification**: The evaluation component uses a basic substring matching for semantic equivalence. A more accurate evaluation would require advanced semantic similarity techniques.

6.  **API Key Security**: While the tool allows providing the API key via command line, it is assumed that users will follow best practices and use environment variables for better security, especially in shared environments.

These assumptions have guided the implementation and define the scope and limitations of the current version of the tool.





## Known Issues

1.  **OpenAI API Key Requirement**: The tool will not function without a valid OpenAI API key. If the key is invalid or missing, `openai.AuthenticationError` will be raised.

2.  **API Rate Limits**: For a large number of reviews, you might encounter OpenAI API rate limit errors. The current implementation does not include explicit rate limit handling or retry mechanisms. If this occurs, you may need to implement a delay between API calls or use a more robust library for API interaction that handles retries automatically.

3.  **Clustering Convergence Warning**: You might occasionally see a `ConvergenceWarning` from `scikit-learn` regarding the number of distinct clusters being smaller than `n_clusters`. This happens when the input attributes are not diverse enough to form the requested number of clusters. This is generally harmless for the current simplified clustering approach but indicates that the `num_clusters` might be too high for the given data.

4.  **Semantic Equivalence in Evaluation**: As noted in the Evaluation Component section, the current semantic equivalence check is a simple substring match. This means that semantically similar but lexically different attributes (e.g., "good smell" vs. "pleasant aroma") might not be correctly identified as matches in the evaluation, leading to a lower reported accuracy than the tool's actual performance in a human assessment.

5.  **Limited Error Handling in `extract_attributes`**: The `extract_attributes` method has a general `try-except` block. More specific error handling for different types of OpenAI API errors (e.g., `RateLimitError`, `APIConnectionError`) could provide more informative feedback.
